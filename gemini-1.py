# rag_with_gemini.py
from google import genai
import json
import numpy as np
import faiss
from typing import List
import time
from dotenv import load_dotenv
import os

# Optional GUI
import gradio as gr

# Lightweight HTTP server for integration
try:
    from fastapi import FastAPI
    from pydantic import BaseModel
    from fastapi.middleware.cors import CORSMiddleware
    import uvicorn
except Exception:
    FastAPI = None  # type: ignore
    BaseModel = object  # type: ignore
    uvicorn = None  # type: ignore

load_dotenv()
MAX_CHARS = 2000
api_key = os.getenv("GENAI_API_KEY") or os.getenv("GEMINI_API_KEY")
def truncate_prompt(prompt: str):
    if len(prompt) > MAX_CHARS:
        return prompt[:MAX_CHARS] + "..."
    return prompt

#Client
client = genai.Client(api_key = api_key)  

# Data
with open("data.json", "r", encoding="utf-8") as f:
    docs = json.load(f)  # list of {id, dialect, text, norm, ...}

with open("law_data.json", "r", encoding="utf-8") as f:
    laws = json.load(f)  # list of {Source, text}

# padding
for law in laws:
    law["dialect"] = law["Source"]  
    law["meaning"] = ""             
    law["is_law"] = True

# padding
for d in docs:
    d["is_law"] = False

# 
all_data = docs + laws

texts = []
for d in all_data:
    if d.get("is_law", False):
        # Embed
        texts.append(f"Law Source: {d['Source']} ||| Text: {d['text']} ||| Type: {d.get('Type','')}")
    else:
        # Embed
        texts.append(f"Dialect: {d['dialect']} ||| Text: {d['text']} ||| Meaning: {d.get('meaning','')}")


# 3) embeddings (batch)
def embed_texts(text_list: List[str], batch_size=100):
    embeddings = []
    for i in range(0, len(text_list), batch_size):
        batch = text_list[i:i+batch_size]
        res = client.models.embed_content(
            model="models/embedding-001",
            contents=batch
        )
        embeddings.extend([e.values for e in res.embeddings])
    return embeddings

embs = embed_texts(texts)

# 4) build FAISS index (L2 / cosine -> normalize for cosine)
d = len(embs[0])
arr = np.array(embs).astype("float32")
# normalize for cosine similarity
faiss.normalize_L2(arr)
index = faiss.IndexFlatIP(d)  # inner product on normalized vectors = cosine
index.add(arr)

# keep mapping id -> original doc
id_map = {i: all_data[i] for i in range(len(all_data))}

# 5) query + retrieval
def retrieve(query: str, k=3):
    q_emb = embed_texts([query])[0]
    q = np.array([q_emb]).astype("float32")
    faiss.normalize_L2(q)
    D, I = index.search(q, k)
    hits = [id_map[idx] for idx in I[0] if idx != -1]
    return hits

def print_typing_effect(text, delay=0.03):
    for ch in text:
        print(ch, end="", flush=True)
        time.sleep(delay)

# 6) call LLM with retrieved context
def answer_with_context(user_q: str):
    try:
        hits = retrieve(user_q, k=10)
        if not hits:
            print("Xin l·ªói, m√¨nh ch∆∞a c√≥ d·ªØ li·ªáu ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.")
            return
        
        context = "\n\n".join(
            [
                (
                    f"Source: {h['Source']}\nText: {h['text']}\nType: {h.get('Type','')}"
                    if h.get("is_law")
                    else f"Dialect: {h['dialect']}\nText: {h['text']}\nMeaning: {h.get('meaning','')}"
                )
                for h in hits
            ]
        )

        user_prompt = f"""
        B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam.
        Ng∆∞·ªùi d√πng h·ªèi: {user_q}

        Th√¥ng tin ƒë∆∞·ª£c cung c·∫•p (l·∫•y t·ª´ c∆° s·ªü d·ªØ li·ªáu, ƒë√¢y l√† ngu·ªìn th√¥ng tin gi√∫p b·∫°n tr·∫£ l·ªùi c√¢u h·ªèi):
        {context}

        Y√™u c·∫ßu:
        1. N·∫øu c√¢u h·ªèi kh√¥ng h·ªèi hay b√†n v·ªÅ v·∫•n ƒë·ªÅ ph√°p lu·∫≠t. Tr·∫£ l·ªùi "T√¥i ch·ªâ gi·∫£i quy·∫øt c√°c v·∫•n ƒë·ªÅ v·ªÅ ph√°p lu·∫≠t"
        2. N·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ t√≠n ƒë√∫ng ƒë·∫Øn hay sai tr√°i c·ªßa lu·∫≠t th√¨ ƒë∆∞a ra c√°c c√°ch ki·ªÉm tra cho ng∆∞·ªùi d√πng. Kh√¥ng tr·∫£ l·ªùi 1 c√°ch ch·∫Øc ch·∫Øn
        3. N·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ n·ªôi dung lu·∫≠t ph√°p v·ªÅ v·∫•n ƒë·ªÅ g√¨ th√¨ s·ª≠ d·ª•ng d·ªØ li·ªáu trong Ng·ªØ c·∫£nh. Kh√¥ng s·ª≠a ƒë·ªïi hay b·ªï sung th√™m
        4. Kh√¥ng th√™m b·∫•t k√¨ l∆∞u √Ω hay nh·∫Øc nh·ªù v·ªÅ n·ªôi dung trong Ng·ªØ c·∫£nh
        5. N·∫øu kh√¥ng th·∫•y d·ªØ li·ªáu trong Th√¥ng tin ƒë∆∞·ª£c cung c·∫•p th√¨ tr·∫£ l·ªùi l√† "M·∫∑c d√π d·ªØ li·ªáu ch∆∞a ƒë∆∞·ª£c cung c·∫•p theo t√¥i bi·∫øt..." r·ªìi th√™m √Ω c·ªßa b·∫°n v√†o
        6. ƒê·ª´ng nh·∫Øc ƒë·∫øn "Th√¥ng tin ƒë∆∞·ª£c cung c·∫•p" m√† t√¥i ƒë√£ g·ª≠i cho b·∫°n trong c√¢u tr·∫£ l·ªùi
        "
        """
        user_prompt = truncate_prompt(user_prompt)

        # print(user_prompt)

        stream = client.models.generate_content_stream(
            model="gemini-2.5-flash",
            contents=user_prompt
        )
        response = ""

        for chunk in stream:
            response += chunk.text

        return response
    except Exception as e:
        print(f"L·ªói khi g·ªçi API: {e}")
        return "Xin l·ªói, m√¨nh g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n."


#Run with GUI 
with gr.Blocks() as demo:
    gr.Markdown("# ü§ñ")
    chatbot = gr.Chatbot()
    msg = gr.Textbox(placeholder="Nh·∫≠p c√¢u h·ªèi v·ªÅ ph√°p lu·∫≠t...")
    clear = gr.Button("X√≥a h·ªôi tho·∫°i")

    def user_message(user_input, chat_history):
        response = answer_with_context(user_input)
        chat_history.append((user_input, response))
        return "", chat_history

    msg.submit(user_message, [msg, chatbot], [msg, chatbot])
    clear.click(lambda: [], None, chatbot, queue=False)

def create_app():
    if FastAPI is None:
        return None

    app = FastAPI()

    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    class Query(BaseModel):
        question: str

    @app.post("/answer")
    def answer(q: Query):
        try:
            text = answer_with_context(q.question) or ""
            return {"answer": text}
        except Exception as e:
            return {"answer": f"L·ªói: {e}"}

    @app.get("/health")
    def health():
        return {"status": "ok"}

    return app


if __name__ == "__main__":
    mode = os.getenv("GEMINI_MODE", "api")  # api | gradio | both
    if mode in ("gradio", "both"):
        demo.launch(server_name="0.0.0.0", server_port=int(os.getenv("GRADIO_PORT", "7860")))
    if mode in ("api", "both") and FastAPI is not None:
        app = create_app()
        uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("GEMINI_PORT", "7861")))